{"cells":[{"cell_type":"markdown","metadata":{"id":"EEmVbCWiQmtp"},"source":["# 0.Import"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51101,"status":"ok","timestamp":1694517006575,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"},"user_tz":-540},"id":"0a_q-JmPAv8i","outputId":"0ae049c9-2b53-4075-b0cc-aae1aeea106d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOfkhSWOA4WS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694517097938,"user_tz":-540,"elapsed":22451,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"}},"outputId":"8b956123-e410-4513-fc45-0cf1e723c06d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: noun_splitter in /usr/local/lib/python3.10/dist-packages (0.0.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from noun_splitter) (1.11.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from noun_splitter) (1.23.5)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from noun_splitter) (7.4.1)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from noun_splitter) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from noun_splitter) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from noun_splitter) (4.66.1)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->noun_splitter) (2.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->noun_splitter) (23.1)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->noun_splitter) (1.3.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->noun_splitter) (1.1.3)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->noun_splitter) (2.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->noun_splitter) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->noun_splitter) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->noun_splitter) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->noun_splitter) (2023.7.22)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.10/dist-packages (0.9.9)\n","Cloning into 'pycon-korea-2020-kb-albert'...\n","remote: Enumerating objects: 41, done.\u001b[K\n","remote: Counting objects: 100% (41/41), done.\u001b[K\n","remote: Compressing objects: 100% (32/32), done.\u001b[K\n","remote: Total 41 (delta 19), reused 19 (delta 8), pack-reused 0\u001b[K\n","Receiving objects: 100% (41/41), 1.66 MiB | 5.04 MiB/s, done.\n","Resolving deltas: 100% (19/19), done.\n","/content/pycon-korea-2020-kb-albert\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["! pip install noun_splitter\n","! pip install torch transformers datasets\n","! pip install python-crfsuite\n","\n","# Download source codes\n","!git clone https://github.com/sackoh/pycon-korea-2020-kb-albert.git\n","%cd pycon-korea-2020-kb-albert/\n","\n","# Install transformers\n","%pip install -q transformers\n","! pip install --upgrade transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4302,"status":"ok","timestamp":1694517046339,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"},"user_tz":-540},"id":"OgAci3Y6uenW","outputId":"497561bb-2f6b-4f45-fd0c-933ece527104"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.10/dist-packages (0.9.9)\n"]}],"source":["! pip install python-crfsuite"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEGsK2ZjQkoz"},"outputs":[],"source":["import json\n","import re\n","import torch\n","import time\n","import logging\n","import torch\n","import pandas as pd\n","import json\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from pathlib import Path\n","from transformers.data.processors.squad import SquadExample, squad_convert_examples_to_features\n","from transformers import AlbertForQuestionAnswering, TrainingArguments, Trainer\n","from transformers import AdamW, get_linear_schedule_with_warmup, AlbertTokenizer\n","from transformers import TFAutoModel, AutoTokenizer, AutoModel\n","from tokenization_kbalbert import KbAlbertCharTokenizer\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.tensorboard import SummaryWriter\n","from sklearn.model_selection import train_test_split\n","from tqdm import trange, tqdm"]},{"cell_type":"code","source":["bar = '='\n","device = torch.device('cuda')\n","print(f'{bar*10}Device INFO{bar*10}')\n","print(f'Device :{device}')\n","print(bar*31)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIbFUp8D2oFQ","executionInfo":{"status":"ok","timestamp":1694517105909,"user_tz":-540,"elapsed":280,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"}},"outputId":"42c06b7a-5930-467e-b17b-ee09785e807f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==========Device INFO==========\n","Device :cuda\n","===============================\n"]}]},{"cell_type":"markdown","metadata":{"id":"GAVmMtEHXsw6"},"source":["# 1.train data 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_Wab1wPQ6OY"},"outputs":[],"source":["def create_examples(input_data1,input_data2, set_type):\n","    is_training = set_type == \"train\"\n","    examples = []\n","\n","    print(\"input_data1 처리 시작\")\n","    filtered_data = [item for item in input_data1 if \"금융\" in item[\"doc_class\"][\"class\"]]\n","\n","    for item in filtered_data:\n","        title = item['doc_title']\n","\n","        for para in item[\"paragraphs\"]:\n","            context_text = para[\"context\"]\n","\n","            for qa in para[\"qas\"]:\n","                qas_id = qa[\"question_id\"]\n","                question_text = qa[\"question\"]\n","                answer_text = qa[\"answer\"][\"text\"]\n","                start_position_character = qa[\"answer\"][\"answer_start\"]\n","                end_position_character = qa[\"answer\"][\"answer_end\"]\n","                answers = []\n","\n","                is_impossible = qa.get(\"is_impossible\", False)\n","\n","                # 정답 텍스트와 시작 위치를 가져와서 딕셔너리 형태로 만듦\n","                answer_dict = {\n","                    \"text\": answer_text,\n","                    \"answer_start\": start_position_character\n","                }\n","\n","                # 만약 is_impossible가 False라면 정답을 answers 리스트에 추가\n","                if not is_impossible:\n","                    answers.append(answer_dict)\n","\n","                example = SquadExample(\n","                    qas_id=qas_id,\n","                    question_text=question_text,\n","                    context_text=context_text,\n","                    answer_text=answer_text,\n","                    start_position_character=start_position_character,\n","                    title=title,\n","                    is_impossible=is_impossible,\n","                    answers=answers\n","                )\n","\n","                examples.append(example)\n","    print(\"input_data1 처리 완료\")\n","\n","    print(\"input_data2 처리 시작\")\n","    for index, row in input_data2.iterrows():\n","      title = row['title']\n","      context_text = row[\"context\"]\n","      qas_id = row[\"question_id\"]\n","      question_text = row[\"question\"]\n","      answer_text = row[\"answer\"]\n","      start_position_character = int(row[\"answer_start\"])\n","      end_position_character = int(row[\"answer_end\"])\n","      answers = []\n","\n","      is_impossible = row.get(\"is_impossible\", False)\n","\n","      # 정답 텍스트와 시작 위치를 가져와서 딕셔너리 형태로 만듦\n","      answer_dict = {\n","          \"text\": answer_text,\n","          \"answer_start\": start_position_character\n","      }\n","\n","      # 만약 is_impossible가 False라면 정답을 answers 리스트에 추가\n","      if not is_impossible:\n","          answers.append(answer_dict)\n","\n","      example = SquadExample(\n","          qas_id=qas_id,\n","          question_text=question_text,\n","          context_text=context_text,\n","          answer_text=answer_text,\n","          start_position_character=start_position_character,\n","          title=title,\n","          is_impossible=is_impossible,\n","          answers=answers\n","      )\n","\n","      examples.append(example)\n","\n","    print(\"input_data2 처리 완료\")\n","\n","\n","    train_examples, test_examples = train_test_split(examples, test_size=0.2, random_state=42)\n","    return train_examples, test_examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVEWB6OiBBRo"},"outputs":[],"source":["def get_train_examples(filename1, filename2):\n","\n","  if filename1 :\n","    path = Path(filename1)\n","\n","    with open(path, \"rb\") as f:\n","      input_data1 = json.load(f)[\"data\"]\n","\n","  if filename2 :\n","    input_data2 = pd.read_csv(filename2)\n","\n","  return create_examples(input_data1,input_data2, \"train\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3A5DHjvBBpJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694517119795,"user_tz":-540,"elapsed":10593,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"}},"outputId":"5c6c3833-d2d4-46f5-ff2b-ae1a57b47aa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_data1 처리 시작\n","input_data1 처리 완료\n","input_data2 처리 시작\n","input_data2 처리 완료\n"]}],"source":["filename1 = \"/content/gdrive/MyDrive/절미네 가족들/1.jiwon/1.data/0.row_data/AI_hub_fin_data.json\"\n","filename2 = \"/content/gdrive/MyDrive/절미네 가족들/1.jiwon/1.data/0.row_data/QA_set_final_ver3.csv\"\n","\n","train_examples, test_examples = get_train_examples(filename1,filename2)"]},{"cell_type":"code","source":["len(train_examples)"],"metadata":{"id":"eu20pIbGwtPZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694517119795,"user_tz":-540,"elapsed":8,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"}},"outputId":"5c27bf21-e408-4616-dc78-46c62eb80ec6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15876"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2812,"status":"ok","timestamp":1694517122603,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"},"user_tz":-540},"id":"Y1qhytFvsdKo","outputId":"d931cdbc-8d95-481d-a954-b45cdd750faa"},"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'KbAlbertCharTokenizer'.\n"]}],"source":["import sys\n","sys.path.append('/content/gdrive/MyDrive/절미네 가족들/1.jiwon/0.github_clone/KB-ALBERT-KO/src/preprocessing/')\n","\n","from noun_splitter import NounSplitter\n","\n","MODEL_PATH = \"/content/gdrive/MyDrive/절미네 가족들/1.jiwon/2.modeling/kb-albert-char-base-v2\"\n","\n","# Convert Unicode path to ASCII bytes\n","ascii_path = \"/content/gdrive/MyDrive/절미네 가족들/1.jiwon/0.github_clone/KB-ALBERT-KO/src/preprocessing/model/np2.crfsuite\".encode(\"utf-8\")\n","\n","# Load noun-splitter\n","noun_splitter = NounSplitter(ascii_path)\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = KbAlbertCharTokenizer.from_pretrained(MODEL_PATH)"]},{"cell_type":"markdown","metadata":{"id":"lg9YQOlm8zBQ"},"source":["# 2.모델 training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzrGZhixBx8s","outputId":"2d4320aa-9de7-4e1c-b95a-1fb937481707","executionInfo":{"status":"ok","timestamp":1693805889197,"user_tz":-540,"elapsed":226391,"user":{"displayName":"31 lab","userId":"06792453909838191097"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["convert squad examples to features: 100%|██████████| 15876/15876 [03:39<00:00, 72.25it/s]\n","add example index and unique id: 100%|██████████| 15876/15876 [00:00<00:00, 455376.33it/s]\n"]}],"source":["max_length = 512\n","doc_stride = 128\n","max_query_length = 64\n","\n","train_features, train_dataset = squad_convert_examples_to_features(\n","    examples=train_examples,\n","    tokenizer=tokenizer,\n","    max_seq_length=max_length,\n","    doc_stride=doc_stride,\n","    max_query_length=max_query_length,\n","    is_training=True,\n","    return_dataset=\"pt\",\n",")\n","\n","# valid_features, valid_dataset = squad_convert_examples_to_features(\n","#     examples=train_examples,\n","#     tokenizer=tokenizer,\n","#     max_seq_length=max_length,\n","#     doc_stride=doc_stride,\n","#     max_query_length=max_query_length,\n","#     is_training=True,\n","#     return_dataset=\"pt\",\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEDkiJNLTA2F","executionInfo":{"status":"ok","timestamp":1693816833036,"user_tz":-540,"elapsed":4853413,"user":{"displayName":"31 lab","userId":"06792453909838191097"}},"outputId":"8e6a98a7-6b43-435c-ecb1-21b93b5fbfc3"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at /content/gdrive/MyDrive/절미네 가족들/1.jiwon/2.modeling/kb-albert-char-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch 0: 100%|██████████| 2536/2536 [18:08<00:00,  2.33it/s]\n","Epoch 1: 100%|██████████| 2536/2536 [18:06<00:00,  2.33it/s]\n","Epoch 2: 100%|██████████| 2536/2536 [18:07<00:00,  2.33it/s]\n","Epoch 3: 100%|██████████| 2536/2536 [18:07<00:00,  2.33it/s]\n","Epoch 4: 100%|██████████| 2536/2536 [18:07<00:00,  2.33it/s]\n","Epoch 5: 100%|██████████| 2536/2536 [18:07<00:00,  2.33it/s]\n","Epoch 6: 100%|██████████| 2536/2536 [18:07<00:00,  2.33it/s]\n","Epoch 7: 100%|██████████| 2536/2536 [18:07<00:00,  2.33it/s]\n","Epoch 8: 100%|██████████| 2536/2536 [18:07<00:00,  2.33it/s]\n","Epoch 9: 100%|██████████| 2536/2536 [18:07<00:00,  2.33it/s]\n","Epoch: 100%|██████████| 10/10 [3:01:13<00:00, 1087.40s/it]\n"]}],"source":["logger = logging.getLogger(__name__)\n","\n","train_batch_size = 16\n","gradient_accumulation_steps = 1\n","num_train_epochs = 10\n","max_grad_norm = 1.0\n","\n","tb_writer = SummaryWriter()\n","\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n","model = AlbertForQuestionAnswering.from_pretrained(MODEL_PATH)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","no_decay = [\"bias\", \"LayerNorm.weight\"]\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","        \"weight_decay\": 0.0,\n","    },\n","    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n","t_total = len(train_dataloader) //  gradient_accumulation_steps * num_train_epochs\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=0, num_training_steps=t_total\n",")\n","\n","logger.info(\"***** Running training *****\")\n","logger.info(\"  Num examples = %d\", len(train_dataset))\n","logger.info(\"  Num Epochs = %d\", num_train_epochs)\n","logger.info(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n","logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","global_step = 1\n","epochs_trained = 0\n","steps_trained_in_current_epoch = 0\n","tr_loss, logging_loss = 0.0, 0.0\n","model.zero_grad()\n","\n","for epoch in trange(epochs_trained, int(num_train_epochs), desc=\"Epoch\", position=0, leave=True):\n","  epoch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch}\", position=0, leave=True)\n","  for step, batch in enumerate(epoch_iterator):\n","\n","    model.train()\n","    batch = tuple(t.to(device) for t in batch)\n","\n","    inputs = {\n","      \"input_ids\": batch[0],\n","      \"attention_mask\": batch[1],\n","      \"token_type_ids\": batch[2],\n","      \"start_positions\": batch[3],\n","      \"end_positions\": batch[4],\n","    }\n","\n","    outputs = model(**inputs)\n","\n","    loss = outputs[0]\n","    loss.backward()\n","\n","    tr_loss += loss.item()\n","    if (step + 1) % gradient_accumulation_steps == 0:\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","      optimizer.step()\n","      scheduler.step()\n","      model.zero_grad()\n","      global_step += 1\n","\n","    tb_writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6IvKsPpXNJc"},"outputs":[],"source":["model.save_pretrained(\"/content/gdrive/MyDrive/절미네 가족들/1.jiwon/2.modeling/KBALBERT_MODEL_V3\")"]},{"cell_type":"markdown","metadata":{"id":"Q1-aTvOH7jAb"},"source":["# 3. 모델 test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKqSeJlu8Dhq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694517233285,"user_tz":-540,"elapsed":976,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"}},"outputId":"e2f7b4d1-3204-4da0-e6a5-344cd8d26180"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at /content/gdrive/MyDrive/절미네 가족들/1.jiwon/2.modeling/kb-albert-char-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'KbAlbertCharTokenizer'.\n"]}],"source":["MODEL_PATH = \"/content/gdrive/MyDrive/절미네 가족들/1.jiwon/2.modeling/kb-albert-char-base-v2\"\n","MODEL_PATH_2 = \"/content/gdrive/MyDrive/절미네 가족들/1.jiwon/2.modeling/KBALBERT_MODEL/pytorch_model.bin\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = AlbertForQuestionAnswering.from_pretrained(MODEL_PATH)\n","model.load_state_dict(torch.load(MODEL_PATH_2))\n","model.to(device)\n","\n","# 텍스트 Tokenizer\n","tokenizer = KbAlbertCharTokenizer.from_pretrained(MODEL_PATH)"]},{"cell_type":"code","source":["test_examples_1 = test_examples[0:100]"],"metadata":{"id":"Vd8JTt6u1S9U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 512\n","doc_stride = 128\n","max_query_length = 64\n","\n","test_features, test_dataset = squad_convert_examples_to_features(\n","    examples=test_examples_1,\n","    tokenizer=tokenizer,\n","    max_seq_length=max_length,\n","    doc_stride=doc_stride,\n","    max_query_length=max_query_length,\n","    is_training=True,\n","    return_dataset=\"pt\",\n",")\n","\n","train_batch_size = 16\n","\n","test_sampler = RandomSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=train_batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vI3hUZEzg0c","executionInfo":{"status":"ok","timestamp":1694517186525,"user_tz":-540,"elapsed":1381,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"}},"outputId":"9debe614-8014-4b99-e6d0-8624fbb4df15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["convert squad examples to features: 100%|██████████| 100/100 [00:01<00:00, 85.38it/s]\n","add example index and unique id: 100%|██████████| 100/100 [00:00<00:00, 261002.12it/s]\n"]}]},{"cell_type":"code","source":["# 모델을 평가 모드로 설정합니다.\n","model.eval()\n","\n","# 예측 및 실제 답변을 저장할 리스트를 준비합니다.\n","predictions = []\n","actuals = []\n","\n","# 데이터로더에서 테스트 데이터셋을 순회합니다.\n","for batch in test_dataloader:\n","    # 입력을 GPU로 전송합니다.\n","    batch = tuple(t.to(device) for t in batch)\n","\n","    with torch.no_grad():\n","        inputs = {\n","            \"input_ids\": batch[0],\n","            \"attention_mask\": batch[1],\n","            \"token_type_ids\": batch[2]\n","        }\n","        # 모델을 실행하여 예측 시작 및 종료 위치를 얻습니다.\n","        outputs = model(**inputs)\n","        start_logits, end_logits = outputs[:2]\n","\n","    # 예측 시작 및 종료 위치의 인덱스를 얻습니다.\n","    start_indices = torch.argmax(start_logits, dim=1)\n","    end_indices = torch.argmax(end_logits, dim=1)\n","\n","    # 예측된 텍스트 답변을 추출합니다.\n","    for i in range(len(batch[0])):\n","        input_ids = batch[0][i].tolist()\n","        pred_answer = tokenizer.decode(input_ids[start_indices[i]:end_indices[i]+1])\n","        actual_answer = tokenizer.decode(input_ids[batch[3][i]:batch[4][i]+1])\n","        predictions.append(pred_answer)\n","        actuals.append(actual_answer)"],"metadata":{"id":"OrqRHtQwy49r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import string\n","import collections\n","\n","def normalize_answer(s):\n","    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","def compute_exact(a_gold, a_pred):\n","    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n","\n","def compute_f1(a_gold, a_pred):\n","    gold_toks = normalize_answer(a_gold).split()\n","    pred_toks = normalize_answer(a_pred).split()\n","    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","    num_same = sum(common.values())\n","    # if len(gold_toks) == 0 or len(pred_toks) == 0:\n","    #     return int(gold_toks == pred_toks)\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(pred_toks)\n","    recall = 1.0 * num_same / len(gold_toks)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","# EM 및 F1 점수를 계산합니다.\n","f1_total = 0\n","exact_total = 0\n","\n","for prediction, actual in zip(predictions, actuals):\n","    f1_total += compute_f1(actual, prediction)\n","    exact_total += compute_exact(actual, prediction)\n","\n","f1 = f1_total / len(predictions)\n","exact_match = exact_total / len(predictions)\n","\n","print(\"F1 Score:\", f1)\n","print(\"Exact Match (EM) Score:\", exact_match)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wg6w6o3miG9G","executionInfo":{"status":"ok","timestamp":1694517248075,"user_tz":-540,"elapsed":270,"user":{"displayName":"무시쿵야","userId":"04890190337292377449"}},"outputId":"91e17202-fcd5-4263-98aa-6c69ec38bb44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score: 0.9194217206811339\n","Exact Match (EM) Score: 0.8513513513513513\n"]}]},{"cell_type":"markdown","source":["# 4.모델 시연"],"metadata":{"id":"EmW1JYa54TOS"}},{"cell_type":"code","source":["test_data = pd.read_csv('/content/gdrive/MyDrive/절미네 가족들/1.jiwon/1.data/0.row_data/QA_set_final_ver3.csv')"],"metadata":{"id":"349305in8Vz3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["key = test_data['question']\n","value = test_data['answer']"],"metadata":{"id":"rTauAX5U8nkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pprint import pprint\n","from transformers import pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import linear_kernel\n","\n","def find_most_similar_context(question, question2, context):\n","    vectorizer = TfidfVectorizer().fit(question2)\n","    tfidf_matrix = vectorizer.transform(question2)\n","    question_vector = vectorizer.transform([question])\n","\n","    # 질문의 TF-IDF 벡터와 모든 질문의 TF-IDF 벡터 사이의 코사인 유사도를 계산\n","    cosine_similarities = linear_kernel(question_vector, tfidf_matrix).flatten()\n","\n","    # 가장 유사한 컨텍스트의 인덱스를 찾음\n","    best_context_index = cosine_similarities.argmax()\n","\n","    return context[best_context_index]\n","\n","qa = pipeline(\"question-answering\", tokenizer=tokenizer, model=model, device=0)"],"metadata":{"id":"CNm0hq6MIMS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"채권이란?\"\n","best_context = find_most_similar_context(question, key, value)"],"metadata":{"id":"4DUGXJ-rIoTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_context"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"7sP7uJHxH02P","executionInfo":{"status":"ok","timestamp":1693824805391,"user_tz":-540,"elapsed":5,"user":{"displayName":"31 lab","userId":"06792453909838191097"}},"outputId":"3f4050b0-28a5-41c3-ca6b-cee081aa9f4e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'채권은 정부, 공공기관, 특수법인 등과 주식회사의 형태를 갖춘 기업이 비교적 거액의 장기자금을 일시에 대량으로 조달하기 위하여 발행하는 일종의 차용증서이다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["question"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"c7krGW8sIafP","executionInfo":{"status":"ok","timestamp":1693824807612,"user_tz":-540,"elapsed":316,"user":{"displayName":"31 lab","userId":"06792453909838191097"}},"outputId":"761bad53-5b2c-4871-b1b7-e6d63d8f8149"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'채권이란?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# 가장 유사한 컨텍스트로 답변 얻기\n","\n","answer = qa({\"question\": question, \"context\": best_context})\n","print(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3XWsOXWzIh2B","executionInfo":{"status":"ok","timestamp":1693824809012,"user_tz":-540,"elapsed":5,"user":{"displayName":"31 lab","userId":"06792453909838191097"}},"outputId":"d8056093-0fcc-454c-d96d-a01bdc9b80cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'score': 0.20731866359710693, 'start': 76, 'end': 86, 'answer': '일종의 차용증서이다.'}\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"nrr1DLUg_9nG"}},{"cell_type":"code","source":["# 한번 따로 넣어서 해보기\n","\n","context = \"\"\"\n","기업들은 생산요소를 구입하여 생산 활동을 함으로써 매출과 이익을 올리고 있다.\n","성공적인 경제활동을 했다면 비용보다 수입이 크게 될 것이다. 기업의 소득은 주주들에게 배당하기도 하고 새로운 사업을 위하여 직접 보유하기도 한다.\n","그럼에도 기업은 현재의 생산을 유지하거나 새로운 사업을 위해서 항상 자금이 필요하다.\n","기업은 생산물 판매가 활발하게 일어나면 시설을 확장하기 위해서 돈이 필요하고, 반대로 판매 수입이 줄어들면 부족한 돈을 빌리게 되므로 통상적으로 자금의 수요자가 된다.\n","가계·기업·정부 모두 수입보다 지출이 많으면 적자가 되고 반대로 수입보다 지출이 적으면 흑자가 된다.\n","흑자 주체는 자신들의 돈을 효과적으로 활용하기 위하여 다양한 방법으로 돈을투자하게 되고 반대로 적자 주체는 필요한 돈을 마련하기 위해서 돈을 빌리게 된다.\n","이와 같이 흑자 주체에서 적자 주체로 돈이 이동하는 것을 금융이라 한다.\n","즉 여유자금이 있는 흑자 주체(저축자: 공급자)가 돈을 빌려주거나 적자 주체(차입자: 수요자)가 돈을 빌리는 것을 금융이라 하고 금융거래가 일어나는 곳을 금융시장이라고 한다.\n","생산·소비·유통·분배와 같은 경제활동이 원활하게 일어나기 위해서는 각 경제주체 간에 일어나는 거래를 뒷받침할 수 있는 돈의 양이 충분해야 하고\n","돈의 흐름을 원활하게 해주는 금융시장이 잘 발달되어 있어야 한다.선진국들은 생산물의 크기도 크지만 경제활동을\n","뒷받침하는 금융시장도 잘 발달되어 있는 데 비해서 개발도상국들은 상대적으로 생산성도 낮고 금융시장도 낙후되어 있다.\n","\"\"\"\n","\n","context2 =  \"\"\"\n","즉 여유자금이 있는 흑자 주체(저축자: 공급자)가 돈을 빌려주거나 적자 주체(차입자: 수요자)가 돈을 빌리는 것을 금융이라 하고 금융거래가 일어나는 곳을 금융시장이라고 한다.\n","\"\"\"\n","\n","answer = qa({\"question\": '경제활동이 원활하게 일어나기 위해 필요한 것은 무엇인가요?', \"context\": context})\n","print(answer)"],"metadata":{"id":"ihLnEMW-nQp1"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4","collapsed_sections":["lg9YQOlm8zBQ"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}